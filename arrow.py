from datasets import load_from_disk, concatenate_datasets
import pickle
import os
import json
import random
from transformers import GPT2TokenizerFast,DataCollatorForLanguageModeling, AutoModelForCausalLM, TrainingArguments, Trainer

data_path = 'The arrow file generated by generate_score.py'
train_set = load_from_disk(os.path.join(data_path, 'train'))



# # 把数据集遍历到一个文件夹里面0,1,2,3.....
path = 'you choose path'

for i in range(len(train_set)):
    filename = path + str(i+1)
    # if not os.path.exists(filename):
    os.makedirs(filename)
    solutions_txt = [train_set[i]['answer_code']]
    gen_solutions_txt = [train_set[i]['gpt2_completed']]
    question_txt = train_set[i]['input_code']
    all_question_txt = train_set[i]['source_code']

    solutions = open(filename + '/' + 'solutions.json','w')
    solutions_array = json.dumps(solutions_txt)
    solutions.write(solutions_array)
    solutions.close()

    gen_solutions = open(filename + '/' + 'gen_solutions.json','w')
    gen_solutions_array = json.dumps(gen_solutions_txt)
    gen_solutions.write(gen_solutions_array)
    gen_solutions.close()

    question = open(filename + '/' + 'question.txt','w')
    question.write(question_txt)
    question.close()

    all_question = open(filename + '/' + 'all_question.txt', 'w')
    all_question.write(all_question_txt)
    all_question.close()
#
    if i % 5000 == 0:
        print('finish {}'.format(i))



















# """将CodeXglue中的train.txt文件遍历为code completion evaluator中用到的json文件"""
# path = 'train.txt'
# with open(path) as f:
#     line = f.readlines()
# json_file = open('2.json' ,'w')
#
# for i in line:
#     item = { }
#     item['code'] = i
#     json_str = json.dumps(item)
#     json_file.write(json_str)
#     json_file.write("\r\n")
# print(item)

# data_path = 'datasets/codegpt_no_RL'
# test_set = load_from_disk(os.path.join(data_path, 'test'))
# json_file = open('answers.json','w')
# for i in test_set:
#     item = { }
#     item["gt"] = i['answer_code']
#     json_str = json.dumps(item)
#     json_file.write(json_str)
#     json_file.write("\r\n")
# json_file = open('predictions.json','w')
# for i in test_set:
#     item = { }
#     item["gt"] = i['gpt2_completed']
#     json_str = json.dumps(item)
#     json_file.write(json_str)
#     json_file.write("\r\n")

# clone_file = open('clone.txt','r')
# fintune_file = open('fintune.txt','r')
# a = clone_file.readlines()
# b = fintune_file.readlines()
# all = []
# for i in range(10000):
#     if a[i] != b[i]:
#         # print(i+1)
#         all.append(i+1)
# print(all.__len__())





